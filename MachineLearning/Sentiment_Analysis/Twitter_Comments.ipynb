{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c213d51",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on twitter comments.\n",
    "### Performing sentiment on Sentiment140 dataset from kaggle.\n",
    "Using NLP(Natural Language Processing) to pre-process the data set and applying algorithm such as Naive Baye and Logistic Regression to check the model accuracy.\n",
    "### Data set contents:\n",
    "\n",
    "    1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "    2. ids: The id of the tweet (2087)\n",
    "    3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    5. user: the user that tweeted.\n",
    "    6. text: the text of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac1f59",
   "metadata": {},
   "source": [
    "## Step-1: Loading, inspection and cleaning of data:\n",
    "Importing all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231ea9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93975df1",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9591433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv data into pandas dataframe.\n",
    "data_set = pd.read_csv(\"Datasets//Twitter_Sentiment_Analysis/training_1.6_million.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1818948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column names for the data set\n",
    "data_set.columns = ['target', 'id', 'date', 'flag', 'user', 'comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e32c5",
   "metadata": {},
   "source": [
    "### Inspecting the data\n",
    "#### Lets have a look at the data.\n",
    "Checking for null values present in the data, as well as removal of unwanted columns if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b542fede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6       0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7       0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8       0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9       0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                            comment  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9          mimismo                          @twittera que me muera ?   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d084994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target     0\n",
       "id         0\n",
       "date       0\n",
       "flag       0\n",
       "user       0\n",
       "comment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for null values\n",
    "data_set.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d81431",
   "metadata": {},
   "source": [
    "As we can see the data is clean of null values, hence we can go ahead with further processing of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e85cb",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b5ee64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                            comment\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all....\n",
       "5       0                      @Kwesidei not the whole crew \n",
       "6       0                                        Need a hug \n",
       "7       0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8       0               @Tatiana_K nope they didn't have it \n",
       "9       0                          @twittera que me muera ? "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting of unwanted columns from the data\n",
    "del data_set['id']\n",
    "del data_set['date']\n",
    "del data_set['flag']\n",
    "del data_set['user']\n",
    "\n",
    "# Lets have a look at our actual data\n",
    "data_set.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5e224",
   "metadata": {},
   "source": [
    "Now our data looks much more clean.\n",
    "Let's proceed with the data pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef10307",
   "metadata": {},
   "source": [
    "#### Using the nltk(natural language tool kit) for NLP on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32891fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dcaa5",
   "metadata": {},
   "source": [
    "#### Creating a set of stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15517dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8115e14",
   "metadata": {},
   "source": [
    "#### Preprocessing on text data\n",
    "    1. Casing - Convert data to upper/lower case.\n",
    "    2. Noise Removal - Eliminate unwanted chars like html tags, punctuatuion marks, special characters, white spaces, etc.\n",
    "    3. Tokenization - Turning tweets into tokens. Words separated by spaces in the text\n",
    "    4. Stopword Removal - Removing the words that do not add meaning to any secntence.\n",
    "    5. Text Normalization(Stemming and Lemmatization) - \n",
    "        i. Stemming - Eliminate the prefix, suffix, infix from a word\n",
    "       ii. Lemmatization - Stemming sometime loses the actual meaning of the word. Lemmatization reduces the inflected word properly by taking its morphological analysis into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b253e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(data):\n",
    "    \n",
    "    # Converting text to lower case\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    data = re.sub(r\"[0-9]\", \"\", data)\n",
    "    \n",
    "    # Removal of urls if present\n",
    "    data = re.sub(r\"https\\S+|www\\S+|http\\S+\", \"\", data, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user name\n",
    "    data = re.sub(r\"@([A-Za-z0â€“9_.]+)\", \"\", data)\n",
    "    \n",
    "    # Removal of special characters\n",
    "    data = re.sub(r\"\\@\\w+|\\#\", \"\", data)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    data = data.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    \n",
    "    # Removing stopwords\n",
    "    data_tokens = word_tokenize(data)\n",
    "    filtered_data = [word for word in data_tokens if word not in stop_words]\n",
    "    \n",
    "    # Remove single letter words\n",
    "    filtered_data = [word for word in filtered_data if len(word) > 1]\n",
    "    \n",
    "    # Stemming\n",
    "    porter_stem = PorterStemmer()\n",
    "    stemmed_data = [porter_stem.stem(word) for word in filtered_data]\n",
    "    \n",
    "    # Lematization\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    lematized_data = [lemmatize.lemmatize(word, 'a') for word in stemmed_data]\n",
    "    \n",
    "    data = \" \".join(lematized_data)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ad257c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a15c4f218f244bbb65a6fa57c31a852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looping through the tweets one by one and calling function pre_process_data for data preprocessing.\n",
    "data_set['comment'] = [pre_process_data(data) for data in tqdm(data_set['comment'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca05f96",
   "metadata": {},
   "source": [
    "Our data preprocessing is almost done.\n",
    "Lets check the modified data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b87a6aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>that bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dive mani time ball manag save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>behav im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                            comment\n",
       "0       0       that bummer shoulda got david carr third day\n",
       "1       0  upset cant updat facebook text might cri resul...\n",
       "2       0       dive mani time ball manag save rest go bound\n",
       "3       0                    whole bodi feel itchi like fire\n",
       "4       0                              behav im mad cant see"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c300a9",
   "metadata": {},
   "source": [
    "Wow, our data looks so much better than before now.\n",
    "Before proceeding, lets check the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b979bd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ea295",
   "metadata": {},
   "source": [
    "Negative tweets are labelled as 0, while as positive tweets are labelled 4. Let's change that to 1 before we perform the vectorization operation on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7242789c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing all the 4's with 1's in the target column\n",
    "data_set['target'] = data_set['target'].replace(4, 1)\n",
    "data_set.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df088c",
   "metadata": {},
   "source": [
    "Wow, verything looks good now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592355b2",
   "metadata": {},
   "source": [
    "## Step-2: Conversion of data\n",
    "### Conversion of textual data into numeric representation, to feed into the model.\n",
    "Importing the required functions to convert the text data into tokens and vectorize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00bb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b59ed",
   "metadata": {},
   "source": [
    "### Vectorize tokens\n",
    "Converting the word tokens to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2099084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = TfidfVectorizer(binary='true')\n",
    "data_new = vectorize.fit_transform(data_set['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0805b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 70811)\t0.17742138934618049\n",
      "  (0, 310802)\t0.4010903980233806\n",
      "  (0, 48104)\t0.506368005287779\n",
      "  (0, 70634)\t0.3521196682769122\n",
      "  (0, 118785)\t0.1946914711431045\n",
      "  (0, 278813)\t0.44135380244808553\n",
      "  (0, 42919)\t0.37482696082312\n",
      "  (0, 308215)\t0.23250403699334615\n"
     ]
    }
   ],
   "source": [
    "print(data_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0bc9d",
   "metadata": {},
   "source": [
    "### Data split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c70b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1280000, 367639)\n",
      "Testing set: (320000, 367639)\n"
     ]
    }
   ],
   "source": [
    "x = data_new\n",
    "y = data_set['target'].values\n",
    "\n",
    "# Splitting of data into train and test sets\n",
    "x_train, x_test, y_train, y_test =  train_test_split(x, y, test_size= 0.2, random_state=44)\n",
    "\n",
    "print('Training set: {}'.format(x_train.shape))\n",
    "print('Testing set: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fac0f",
   "metadata": {},
   "source": [
    "## Step-3: Selection and training of model\n",
    "### Train the model with the prepared data\n",
    "Importing the selecting models and functions required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fc2784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c44d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes\n",
    "classify_nb = BernoulliNB()\n",
    "classify_nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e2165b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "classify_lg =LogisticRegression(fit_intercept=False, max_iter=100)\n",
    "classify_lg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989490d",
   "metadata": {},
   "source": [
    "## Step-4: Testing the model\n",
    "### Performance of the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "557fb834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of Bernoulli Naive Bayes\n",
    "prediction = classify_nb.predict(x_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "283b5070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernouli Naive Bayes:\n",
      "\n",
      "Accuracy score: 0.76821875\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77    159759\n",
      "           1       0.77      0.77      0.77    160241\n",
      "\n",
      "    accuracy                           0.77    320000\n",
      "   macro avg       0.77      0.77      0.77    320000\n",
      "weighted avg       0.77      0.77      0.77    320000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[122097  37662]\n",
      " [ 36508 123733]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bernouli Naive Bayes:\")\n",
    "\n",
    "print(\"\\nAccuracy score: {}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"\\nClassification Report:\\n{}\".format(classification_report(y_test, prediction)))\n",
    "print(\"\\nConfusion Matrix:\\n{}\".format(confusion_matrix(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c6931c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance of Logistic Regression\n",
    "prediction = classify_lg.predict(x_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c97aaf",
   "metadata": {},
   "outputs": [{
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "\n",
      "Accuracy score: 0.777184375\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78    159759\n",
      "           1       0.78      0.78      0.78    160241\n",
      "\n",
      "    accuracy                           0.78    320000\n",
      "   macro avg       0.78      0.78      0.78    320000\n",
      "weighted avg       0.78      0.78      0.78    320000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[123704  36055]\n",
      " [ 35246 124995]]\n"
     ]
    }],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "\n",
    "print(\"\\nAccuracy score: {}\".format(accuracy_score(prediction, y_test)))\n",
    "print(\"\\nClassification Report:\\n{}\".format(classification_report(y_test, prediction)))\n",
    "print(\"\\nConfusion Matrix:\\n{}\".format(confusion_matrix(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1170e1",
   "metadata": {},
   "source": [
    "Here we see, using Logistic Regression model, we get a higher accuracy.\n",
    "The accuracy can be increased by further fine tuning the data along with selection of more suitable algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16134e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
